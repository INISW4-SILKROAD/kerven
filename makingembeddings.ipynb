{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## real.json 정보 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 03:52:15,979 - INFO - Dataset loaded: 79918 samples\n",
      "2024-06-11 03:52:16,000 - INFO - Using device: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://dl.fbaipublicfiles.com/dino/dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\n",
      "Checkpoint is already here.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 03:52:17,001 - WARNING - No valid checkpoint found: [Errno 2] No such file or directory: '/mnt/e/data/image/checkpoint_embeddings.npz'\n",
      "Inference:   8%|▊         | 205/2498 [29:58<6:25:08, 10.08s/it]2024-06-11 04:22:24,463 - INFO - Saving checkpoint...\n",
      "2024-06-11 04:22:24,738 - INFO - Checkpoint saved to /mnt/e/data/image/checkpoint_embeddings.npz\n",
      "Inference:  16%|█▌        | 393/2498 [1:00:05<6:35:33, 11.27s/it]2024-06-11 04:52:31,318 - INFO - Saving checkpoint...\n",
      "2024-06-11 04:52:31,718 - INFO - Checkpoint saved to /mnt/e/data/image/checkpoint_embeddings.npz\n",
      "Inference:  23%|██▎       | 568/2498 [1:30:13<5:46:03, 10.76s/it]2024-06-11 05:22:40,178 - INFO - Saving checkpoint...\n",
      "2024-06-11 05:22:40,581 - INFO - Checkpoint saved to /mnt/e/data/image/checkpoint_embeddings.npz\n",
      "Inference:  29%|██▉       | 736/2498 [2:00:17<6:17:12, 12.84s/it]2024-06-11 05:52:46,425 - INFO - Saving checkpoint...\n",
      "2024-06-11 05:52:46,961 - INFO - Checkpoint saved to /mnt/e/data/image/checkpoint_embeddings.npz\n",
      "Inference:  36%|███▌      | 894/2498 [2:30:27<5:20:54, 12.00s/it]2024-06-11 06:22:55,500 - INFO - Saving checkpoint...\n",
      "2024-06-11 06:22:56,043 - INFO - Checkpoint saved to /mnt/e/data/image/checkpoint_embeddings.npz\n",
      "Inference:  42%|████▏     | 1046/2498 [3:00:28<5:13:23, 12.95s/it]2024-06-11 06:52:56,739 - INFO - Saving checkpoint...\n",
      "2024-06-11 06:52:57,464 - INFO - Checkpoint saved to /mnt/e/data/image/checkpoint_embeddings.npz\n",
      "Inference: 100%|██████████| 2498/2498 [3:23:39<00:00,  4.89s/it]  \n",
      "2024-06-11 07:15:56,052 - INFO - Saving final embeddings...\n",
      "2024-06-11 07:15:56,839 - INFO - Final embeddings saved to /mnt/e/data/image/embeddings11.npz\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "import time\n",
    "from torchvision import transforms\n",
    "from oml.datasets.base import DatasetWithLabels\n",
    "from oml.models import ViTExtractor\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(\"EmbeddingsLogger\")\n",
    "\n",
    "# Custom dataset class with index information and image transforms\n",
    "class CustomImageLabeledDataset(DatasetWithLabels):\n",
    "    def __init__(self, df, dataset_root, transform=None):\n",
    "        super().__init__(df=df, dataset_root=dataset_root)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        item = super().__getitem__(idx)\n",
    "        item['indices'] = idx\n",
    "        item['id'] = self.df.iloc[idx]['id']\n",
    "        item['gid'] = self.df.iloc[idx]['gid']\n",
    "\n",
    "        # Apply the transform to the original image\n",
    "        if self.transform:\n",
    "            item[\"input_tensors\"] = self.transform(item[\"input_tensors\"].cpu().permute(1, 2, 0).numpy())\n",
    "\n",
    "        return item\n",
    "\n",
    "# Define the image transformations (resize to 224x224)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load dataset and initialize model\n",
    "dataset_root = \"static\"\n",
    "df_train = pd.read_json(f\"{dataset_root}/real.json\")\n",
    "logger.info(f\"Dataset loaded: {df_train.shape[0]} samples\")\n",
    "\n",
    "# Check if a GPU is available, and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Load pretrained model and move to GPU\n",
    "try:\n",
    "    model = ViTExtractor.from_pretrained(\"vits16_dino\").to(device)\n",
    "    model.eval()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading model: {e}\")\n",
    "    raise\n",
    "\n",
    "# Initialize dataset and data loader\n",
    "try:\n",
    "    dataset_root = \"/mnt/e/data/image\"\n",
    "    train_dataset = CustomImageLabeledDataset(df_train, dataset_root=dataset_root, transform=transform)\n",
    "    full_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error initializing dataset or dataloader: {e}\")\n",
    "    raise\n",
    "\n",
    "# Initialize lists and variables\n",
    "embeddings_list = []\n",
    "labels_list = []\n",
    "id_list = []\n",
    "gid_list = []\n",
    "image_paths_list = []\n",
    "\n",
    "# Initialize timer for checkpointing\n",
    "checkpoint_interval = 30 * 60  # 30 minutes in seconds\n",
    "last_checkpoint_time = time.time()\n",
    "checkpoint_path = f\"{dataset_root}/checkpoint_embeddings.npz\"\n",
    "\n",
    "# Load checkpoint if available\n",
    "def load_checkpoint():\n",
    "    try:\n",
    "        checkpoint_data = np.load(checkpoint_path)\n",
    "        embeddings_list.append(checkpoint_data['embeddings'])\n",
    "        labels_list.append(checkpoint_data['labels'])\n",
    "        id_list.append(checkpoint_data['ids'])\n",
    "        gid_list.append(checkpoint_data['gids'])\n",
    "        image_paths_list.extend(checkpoint_data['image_paths'])\n",
    "        return len(image_paths_list)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"No valid checkpoint found: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Function to save checkpoint\n",
    "def save_checkpoint():\n",
    "    logger.info(\"Saving checkpoint...\")\n",
    "    if embeddings_list:\n",
    "        all_embeddings = np.concatenate(embeddings_list, axis=0)\n",
    "        all_labels = np.concatenate(labels_list, axis=0)\n",
    "        all_ids = np.concatenate(id_list, axis=0)\n",
    "        all_gids = np.concatenate(gid_list, axis=0)\n",
    "        np.savez(checkpoint_path, embeddings=all_embeddings, labels=all_labels, ids=all_ids, gids=all_gids, image_paths=image_paths_list)\n",
    "        logger.info(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "# Resume from the last checkpoint\n",
    "processed_images = load_checkpoint()\n",
    "start_batch = processed_images // 32\n",
    "\n",
    "# Inference Loop\n",
    "with torch.no_grad():\n",
    "    for batch_num, batch in enumerate(tqdm(full_loader, total=len(full_loader), desc=\"Inference\")):\n",
    "        if batch_num < start_batch:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            inputs = batch[\"input_tensors\"].to(device)\n",
    "            labels = batch[\"labels\"]\n",
    "            indices = batch[\"indices\"]\n",
    "            ids = batch[\"id\"]\n",
    "            gids = batch[\"gid\"]\n",
    "\n",
    "            embeddings = model(inputs)\n",
    "\n",
    "            # Transfer tensors to CPU\n",
    "            embeddings_cpu = embeddings.cpu().numpy()\n",
    "            labels_cpu = labels.cpu().numpy()\n",
    "            ids_cpu = np.array(ids)\n",
    "            gids_cpu = np.array(gids)\n",
    "            batch_image_paths = df_train.iloc[indices]['path'].values\n",
    "\n",
    "            embeddings_list.append(embeddings_cpu)\n",
    "            labels_list.append(labels_cpu)\n",
    "            id_list.append(ids_cpu)\n",
    "            gid_list.append(gids_cpu)\n",
    "            image_paths_list.extend(batch_image_paths)\n",
    "\n",
    "            # Check if it's time to save a checkpoint\n",
    "            if time.time() - last_checkpoint_time >= checkpoint_interval:\n",
    "                save_checkpoint()\n",
    "                last_checkpoint_time = time.time()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing batch {batch_num}: {e}\")\n",
    "\n",
    "# Final save after processing all batches\n",
    "logger.info(\"Saving final embeddings...\")\n",
    "try:\n",
    "    all_embeddings = np.concatenate(embeddings_list, axis=0)\n",
    "    all_labels = np.concatenate(labels_list, axis=0)\n",
    "    all_ids = np.concatenate(id_list, axis=0)\n",
    "    all_gids = np.concatenate(gid_list, axis=0)\n",
    "    embeddings_path = f\"{dataset_root}/embeddings11.npz\"\n",
    "    np.savez(embeddings_path, embeddings=all_embeddings, labels=all_labels, ids=all_ids, gids=all_gids, image_paths=image_paths_list)\n",
    "    logger.info(f\"Final embeddings saved to {embeddings_path}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error saving final embeddings: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
